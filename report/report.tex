\documentclass[conference]{IEEEtran}
\usepackage[pdftex]{graphicx}
\usepackage{cite}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{epstopdf}
\usepackage{caption}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
% Do not put math or special symbols in the title.
\title{Multiagent Coordination in Roombas: From the perspective of
    Reinforcement Learning and Neuroevolution}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations

\author{ 
Jimmy Xin Lin \\
Department of Computer Science\\
the University of Texas at Austin\\
Austin, TX 78712 \\
\texttt{jimmylin@cs.utexas.edu} \\
\and
Barry Feigenbaum \\
Department of Computer Science\\
the University of Texas at Austin\\
Austin, TX 78712 \\
\texttt{baf@cs.utexas.edu} \\
}
% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
    % aim of this paper
    This paper presents our research about the reinforcement learning approach
    and the neuroevolution approach, by which the crumb collection task can be
    more effectively and efficiently solved with communication and
    coordination between multiple agents under the simulated Roombas
    environment.
    % literature
    The preliminary literature part gives a brief overview about how existing
    works fulfill the coordination under general multiagent environments.
    % results
    The initial setup experimentation shows our works about the
    learned agents that simulate greedy strategies.
    The key things ... are shown in the following multiagent experiments.
    % important conclusion
    It is observed from our experiments that .
\end{abstract}

\IEEEpeerreviewmaketitle



\section{Introduction}
% utility of roomba and some literatures about its usage in real world
In the past years, Roomba Vacuum has gained its popularity in the industry of
domestic services.  Most of existing studies about Roomba (iRobots) is to 
qualitatively investigate its utility in the home as a single autonomous
domestic service provider. In the contrast, our interests focus on the working
efficacy of Roomba agents under a decentralized system.

% Decentralized system
The decentralized decision making has a long history, originated from the team
thoery (\hspace*{-0.85mm} \cite{marschak1955elements, radner1962team,
    radner1959application, ho1972team, tsitsiklis1985complexity}),
where the decisions made by team members need to contribute to the fulfillment
of global objectives. However, the individual members have only partial
information about the entire system, i.e. limited knowledge of common goals
and global states. This motivates the need for coordination because agents
have to share resources and expertise required to achieve their goals.
Researchers in the field of Distributed Artificial Intelligence (DAI) have
been developing efficient mechanisms to coordinate the activities of multiple
autonomous agents (\hspace*{-1.7mm}\cite{weiss1999multiagent, huhns2012distributed}). 
Specifically, previous works for the multiagent coordination 
include using sophisticated information to exchange protocols, investigating
heuristics for negotiation, and developing formal models of possibilities of
conflict and cooperation among agent interests. 

% Reinforcement learning
Reinforcement learning has been widely used as the most useful techniques for
autonomous game playing (Atari, Pacman, and Angry Bird) and intelligent task
fulfillment either for single-agent or multi-agent environment. 

% Neuroevolution
Neuroevolution.

% Our aim and contributions in this paper
In this paper, we
investigate various multiagent coordination techniques under the framework of
reinforcemnet learning and neuroevolution could improve the working efficacy
of Roomba in the task of cleaning the floor. 
Our contributions include: 
(i) set up the raw Roomba System. 
(ii) implemented reinforcement learning mechanism and fixed up the default
neuroevolution mechanism. 
(iii) design sensors and their representations for multiagent
communication and coordination. 
(iv) compare performances of the agents learned through various approaches and
settings.
% TODO: more contribution or more specific descriptions

% outline of this paper
The remained part of this paper is organized as follows. 
In section \ref{section:literature}, a detailed presentation about the
existing  reinforcement learning treatments and the neuroevolution treatments
for multiagent systems.  
Section \ref{section:environment} describes in detail the virtual Roomba
environment, under which our experiments proceed.  
Preliminary experiments that both serve as baseline intelligent agents and
verify the correct system setup are indicated in the section
\ref{section:setup}. 
The experiments that demonstrate our achievements on multiagent coordination
are articulated in the section \ref{section:realexpo}.
We summarize our conclusions and discuss some promising future works in the
section \ref{conclusion}. TODO: need to update!

\begin{figure}[!t]
\centering
\includegraphics[width=2.1in,height=2.1in]{./figures/irobot_roomba.jpg}
\caption{A exemplar Roomba robot in the real world.}
\label{roomba:world}
\end{figure}


\section{Technical Literatures} \label{section:literature}
This section focuses on the summary of previous works about how multiagent
coordination can be incorporated in the reinforcement learning technique and the
neuroevolution tehnique.

\subsection{Multiagent Planning as Optimization}
Distributed Constraint Optimization Problems (DCOPs) are problems where agents
need to coordinate their value as- signments to maximize the sum of the
resulting constraint utilities.

Dynamic Distributed Constraint Optimization Problems (Dynamic DCOPs) are
introduced to model dynamically changing multiagent coordination problems,
where a dynamic DCOP is a sequence of static DCOPs, each of which partially
different from the DCOP preceding it.
The formulation of Dynamic DCOPs allow us to model problems that can not be assumed
to be static. Dynamic DCOPs are especially advantageous for those problems
that change so frequently that by the time a DCOP solver has found a solution
it is already obsolete. A series of asynchronous algorithms have been proposed
to solve the Dynamic DCOP: Abstract Distributed Constraint Optimization
(DCOP) \cite{maheswaran2004taking}, ADOPT \cite{modi2005adopt}, Support-Based Distributed
Optimisation (SBDO) \cite{billiau2012sbdo}, 

\subsection{Reinforcement Learning Based Coordination}
%######################################################################
%  MDP, DEC-POMDP, and ND-POMDP
Factored Markov Decision Process (Factored MDP) is a common model-based
reinforcement learning formulation for cooperative multiagent dynamic systems.
This framework simply views the entire multiagent system as a single, large
MDP, which can be represented in a factored way using a dynamic Bayesian
network (DBN) \cite{guestrin2001multiagent}. 
It implies that the action space of the resulting MDP is the exponential union
of action space of the entire set of agents. 
One effective approach is to employ factored linear value functions as an
approximation to the joint value function, which can be solved simply by a
single linear program \cite{guestrin2001multiagent}. 
A large collection of algorithms are developed based on the factored MDP model. 
These algorithms have a parameterized, structured representation of a policy
or value function, by which agents coordinate both their action selection
activities and their parameter updates and then determine a jointly optimal
action without explicitly considering every possible action in their
exponentially large joint action space \cite{guestrin2002coordinated}. 
However, the factored MDP may not be applicable to the real application due to
its large search space and associated complexity.
To address this problem, an accelerated gradient method comes up with
$\mathcal{O}(\epsilon^{-1})$ rate of convergence for solving the distributed
multi-agent planning with Factored MDPs \cite{suegeo2011accgrad}.




%######################################################################
% Algorithm: DCOP and its variants
\cite{zhang2011coordinated} This paper presents a
model-free, scalable learning approach that synthesizes
multi-agent reinforcement learning (MARL) and distributed
constraint optimization (DCOP). By exploiting
structured interaction in ND-POMDPs, our approach
distributes the learning of the joint policy and employs
DCOP techniques to coordinate distributed learning to
ensure the global learning performance. Our approach
can learn a globally optimal policy for ND-POMDPs
with a property called groupwise observability. Experimental
results show that, with communication during
learning and execution, our approach significantly outperforms
the nearly-optimal non-communication policies
computed offline.

\cite{nguyen2014decentralized} Existing work typically assumes that the prob-
lem in each time step is decoupled from the problems in other time steps,
which might not hold in some applications. Therefore, in this paper, we make
the following contributions: (i) We introduce a new model, called Markovian
Dynamic DCOPs (MD-DCOPs), where the DCOP in the next time step is a function
of the value assignments in the current time step; (ii) We introduce two
distributed reinforcement learning algo- rithms, the Distributed RVI
Q-learning algorithm and the Dis- tributed R-learning algorithm, that balance
exploration and exploitation to solve MD-DCOPs in an online manner; and (iii)
We empirically evaluate them against an existing multi- arm bandit DCOP
algorithm on dynamic DCOPs.
%######################################################################


\cite{zhang2013coordinating}

\cite{banerjee2012sample}

\cite{kraemer2012informed}

\cite{boukhtouta2011adaptive}
Complex problems involving multiple agents exhibit varying degrees of
cooperation. The levels of cooperation might reflect both differences in
information as well as differences in goals. In this research, we develop a
general mathematical model for distributed, semi-cooperative planning and
suggest a solution strategy which involves decomposing the system into
subproblems, each of which is specified at a certain period in time and
controlled by an agent. The agents communicate marginal values of resources to
each other, possibly with distortion. We design experiments to demonstrate the
benefits of communication between the agents and show that, with
communication, the solution quality approaches that of the ideal situation
where the entire problem is controlled by a single agent.

\cite{sen1994learning}

\subsection{Neuroevolution for Multiagent Coordination}
\textbf{Barry:}
Add literatures of Neuroevolution here...

ATA: barbarian


\section{The Roomba Environment} \label{section:environment}
To the best of our knowledge, the OpenNERO platform is the best choice for
cheap software simulations. 
The OpenNERO is an open-source platform for artificial intelligence
research and education \cite{karpov2008opennero}. 
The public release of the OpenNERO contains an underdeveloped implementation
of Roomba environment, which supports the communication and coordination of
multiple robots.

\subsection{Invariant Structures}
% intro to roomba
The Roomba environment is a virtual computer lab with crumbs distributed on
the floor.
In this virtual lab, there are four classes of objects: agents, crumbs, walls,
decorations. 
Vacuum cleaner agents, shown as grey cylinders in Fig. \ref{roomba:world}, are
supposed to collect the static crumbs that are labelled as blue cubes.  
The agents will be rewarded if they move to a place where there are some
crumbs. 
Walls are also set up as the boundaries of the computer lab, such that
agents are not allowed to move beyond the walls and no pellets can be placed
outside the walls.
Other decorative objects within the virtual environment include tables, chairs, and
computers. For simplicity at the moment, these decorations only serve as
physically transparent decorations, which means they do not block agents' movements.

% controller
As shown in the Fig. \ref{roomba:world}, the small window floating on the top right
is the controller that masters the type of scripted AI agents to load in, the
number of agents, and particular commands that impact the progress of the
Roombas simulation (Pause/Resume, Add/Remove Robots, Exit). On each run of the
simulation, only one particular type of AI agents are allowed to be loaded.
Note also that the type of AI agents employed cannot be switched to the other
one during the intermediate process of crumb collection task. 
If one needs to switch to the other type of AI agents, the running agents have
to be removed and then the user is able to effectively load the desired AI agents.

%  movement of agents 
In the Roombas environment, the movements of vacuum cleaner agents are not
constrained by four directions (left, right, forward, backward). Instead,
agents are able to move towards all directions and each moving action is
denoted as a continuous radius value. Besides, all agents move in a
synchronous way. Agents are allowed to make their movements of the following
step if and only if all agents have completed their movements in the last step.

% placement of pellets 
Three different modes (MANUAL, RANDOM, and CLUSTER) are employed to specify
the placing position of each individual crumb.
In the MANUAL mode, one pellet is deterministically placed at a user-specified
position. 
In the RANDOM mode, the placement of one pellet is totally randomized; the
environment will throw a rejection and repeat the random generation, if an
invalid position is yielded. The last one is the CLUSTER mode, where the
position of pellets are partially randomized. In this mode,
environment samples the position of one pellet from a gaussian distribution
whose centers and spreads at $x$ and $y$ coordinates are specified. 
In this paper, the specifications for the placement of crumbs are generated
from the starting CLUSTER-mode experiment and remained invariant by
employing MANUAL mode in all experiments afterwards.

% learning process
The learning process designed in the Roomba environment is as follows. 
The physically dynamic component, consisting of robots and pellets, will be
reset to their initial status once one episode expires. 
Except the spatial positions, all other properties of agents are allowed to
preserve through two adjacent episodes, for example, the accumulated Q values
from previous episodes. 
Note that the default configuration for one episode is 100 steps. That is,
for every 100 steps, all agents will be placed back to their birth places no
matter how many pellets they have collected and all crumbs are set at
their specified locations, regardless of whether these crumbs exist at the end
of last episode. 

\begin{figure}[!t]
\centering
\includegraphics[width=3.2in,height=2.1in]{./figures/roombas/roomba2.png}
\caption{Overall Picture of the Roomba Environment}
\label{roomba:world}
\end{figure}

\subsection{Practical Issues}
% sensors (default setting for greedy agents)
What agents are allowed to perceive from their local views is one practical
issue that matters the most. 
This is important because both of the design of sensors and their
representations have significant impact on the learning outcome of a team. 
The default implementation of Roomba environment allows each agent to sense
all crumbs on the floor about their positions, existence status, and even
rewards. 
In this sense, agents are able to perceive limitless information from the
computer lab. 
In addition, the spatial information of other collaborated robots
is available for each individual agent, as well as the user-defined working
status of these teammates (e.g. a history of previous positions and
movements).  Although the Roomba environment provides a large design space for
sensors, it is a practical treatment to start from a small number of simple
sensors. For example, a combination of the bumping status, the position of its
own, and the location of closest crumb suffice for an agent to learn a greedy
strategy, as illustrated in the initial setup experimentation.  A balance is
struck between the amount of information available to each agent and the
associated cost.

% rewards
The reward design is another big issue for
configuring the Roomba environment. By default, the only reward being set up
for agents is when they successfully collect some pellets on the floor. In
order to facilitate the learning of agents, an penalty for being alive is
supposed to be incorporated to the reward system. That is, agents should
receive some negative rewards, typically a very small quantity, for each step
they move. Similarly, penalties can also be granted to the collisions between
roombas, bumping of agents towards the world boundaries, and repetitive
movements around an area.


\subsection{Expected Multiagent Behaviors}
TODO: add discussion about the Expected Multiagent Behaviors here...

Work Balance / competition avoidance. 

Collision avoidance. 

\section{Our Approaches}
This section articulates, in a formal way, our original ideas or design
priciples to advance the cleaning efficacy of the Roomba environment from the
multiagent perspective.

\subsection{Baseline Agents} 
Agents with greedy strategy simply approaches to the direction where the
closest pellet to it is there.

TODO: explain Random greedy

\subsection{Tiled Tabular Q-learning} 
TODO: description of TTQ

The motives to use Tiled Tabular Q-learning include
(i) verifying that we have set up the environment correctly for general
reinforcement learning techniques. 
(ii) illustrating the benchmark (reinforcement learning) intelligence for the
crumb collection problem.

\subsection{High-level Reinforcement Learning} 
TODO: any original idea goes here.

\subsection{Neuroevolution} 


\section{Experiments} \label{section:setup}
This section presents our research investigation in two threads: the
reinforcement learning thread and the neuroevolution thread.



\subsection{Tiled Tabular Q-learning}

\begin{figure}[!t]
\centering
\includegraphics[width=3.0in]{./figures/RL/init_setup1.eps}
\caption{Q-learning with various simple sensors. (i) Red: G-random curve
    represents the greedy agent with $0.1$ probability to make a random
    decision, which serve as the benchmark agents. 
    (ii) (iii) (iv) agents receive various collection of sensors for their
    decision making of each step. Q-noCrumb, Q-default, and Q-oneMate
    represent the learning performances of agents that receive $S_{nc}$,
    $S_{d}$, and $S_{om}$ respectively.
} 
\label{fig:RL_init}
\end{figure}

% preliminary results
Before diving into the investigation of multiagent coordination, we take 
preliminary experiments to investigate the impact of various collections of
simple sensors and the effects of various discounting factors on the outcome
of the tiled tabular Q-learning.

TODO: explain some other constans (Number of agents, pellets, )


The sensors designed for the local perspective of each agent are as follows. 
\begin{align}
    S_{nc} = \left( \begin{array}{c}
      self.position.x \\
      self.position.y 
  \end{array} \right)
    S_{d} = \left( \begin{array}{c}
      self.position.x \\
      self.position.y \\
      closestCrumb.x \\
      closestCrumb.y 
  \end{array} \right)
    \nonumber
\end{align}
\begin{align}
        S_{om} = \left( \begin{array}{c}
      self.position.x \\
      self.position.y \\
      closestCrumb.x \\
      closestCrumb.y  \\
      DistToClosestMate.x \\
      DistToClosestMate.y 
  \end{array} \right)
        \nonumber
\end{align}
Note that we discretize all real-value sensors by using the tiling
techniques mentioned in the previous section. 

The learning outcomes derived from these sensors are compared in the
Fig. \ref{fig:RL_init}. 
Observations tell us that the positional information of the closest crumb
tremendously contributes to the quality of agents' learned policies. 
In addition to that, the relative position to the closest collaborated agent
indeed hinder the Q-learning process, such that the communicative agents (black
curve) performs worse than the team that do not monitor collaborators'
positions (light-blue curve) during the observed episodes. One exciting
discovery behind the figure is the narrow performance gap between the team of
tiled tabular Q-learning agents and that of greedy-random agents. It turns out
that the collective works made by naive Q-learning agents even outperform the
team with greedy strategies.

\begin{figure}[!t]
\centering
\includegraphics[width=3.0in]{./figures/RL/init_setup2.eps}
\caption{Q-learning with various discounting factors. (i) Red: G-random curve
    represents the greedy agent with $0.1$ probability to make a random
    decision, which serve as the benchmark agents. 
    (ii) (iii) (iv) the tilted tabular Q-learning with discounting factors
    $\gamma = 0.1, 0.5, 0.8$ for Q-Discount-low, Q-Discount-med, and
    Q-Discount-high respectively.}
\label{fig:RL_init2}
\end{figure}

The Fig. \ref{fig:RL_init2} shows the effects that various discounting factors
will bring to the outcome of tilted tabular Q-learning. It turns out that the
best learning outcome came from the discounting factor $\gamma = 0.5$ under
the given setting. On top of that, this chart also indicates the negative
effects of too large or too small discounting factors, by which worse learning
outcome arises. Nevertheless, it can be concluded that the team formed by
tabular Q-learning agents cannot never outperform the team whose members
employ the greedy strategy, regardless of how the discounting factor is set.

% multiagent coordination
\subsection{High-level Reinforcement Learning}
After the preliminary experiments, it is natural to investigate the problem of how
to incorporate effective coordinations for this particular multiagent system.


%#########################################################################
\subsection{Neuroevolution}


Real-time NEAT does not use generations. However, the search space is eventually depleted of crumbs and must be reset. Simulations are divided into \textit{episodes} which last a fixed interval, and  each episode begin by distributing agents and crumbs around the environment. Agents' brains are continually swapped out during an episode according to the rtNEAT algorithm.

Experiments using the original Roomba configuration yielded poor results. Agents performance did not increase over time.  Even after hundred of episodes, no evidence of learning occurred. This section discusses the steps taken to get agents learning. These results serve as a yardstick for later experiments on multiagent coordination.

In order to facilitate learning, a larger population size is necessary. However, anything more than a handful of Roombas quickly picks up all of the crumbs, even when moving in random directions. To combat this, Roombas are reduced to 1/3 of their original radius. Even so, they often manage to collect 
all of the available crumbs before time expires. When this has been observed, the agents' speed has been capped at different levels. Furthermore, the total time taken to collect all crumbs has been recorded. Even when two experiments collect all of the crumbs, these times can be compared as a secondary performance metric.

%Populations size

\begin{figure}[!t]
\centering
\includegraphics[width=3.0in,height=2.0in]{./figures/neroevolution/pop_size.png}
\caption{Effect of population size on learning}
\label{neroevolution:pop_size}
\end{figure}




\subsubsection{Fitness}
Originally, an agent's fitness was based purely on collecting crumbs. Various other mechanisms were considered, including ability to obstacles and avoid other agents. Experimental evidence showed the best results came from making the fitness gain from crumbs very high, or by moderately penalizing fitness for colliding with walls, resulting in the following:


\[ fitness = \phi * n_{crumbs} - \psi * n_{collisions}\]

Where on $n_{curmbs}$ is the number of crumbs collected and $n_{collisions}$ the number of wall collisions over the course of an episode. 
 
Interestingly, using these two mechanisms together resulted in worse performance than either on their own, as if simultaneously increased reward and penalty negated each others effects. 
Experimentally derived results show $\phi = 1, \psi = 0.05$ as good choices (Fig. \ref{neroevolution:fitness}). 


\begin{figure}[!t]
\centering
\includegraphics[width=3.0in,height=2.0in]{./figures/neroevolution/reward.png}
\caption{Comparison of fitness functions}
\label{neroevolution:fitness}
\end{figure}

%Baseline
In all of our experiments, we compare the results of neuroevolution against hardcoded search behavior as a baseline. We use a simple greedy algorithm - each agent always moves directly towards the nearest crumb. Obviously, this is not an optimal algorithm, and we hope to show that neroevolution can produce a superior result. In particular, the greedy algorithm's agents do not coordinate their efforts. We hope that our evolved agents will learn to better cooperate in order to achieve superior results.

%Single sensor
As a starting point, we try to match the behavior of the greedy algorithm using the simplest possible network. A single sensor detects the nearest crumb. The network receives the angular direction to that crumb as input, and outputs the angular direction the roomba should turn to face. In order to emulate the behavior of the greedy algorithm, the network must map the input unmodified to the single output. Unsurprisingly, the network learns this behavior very quickly.

%2 sensor better than script
In order to improve over the greedy algorithm, it suffices to add just one more sensor. This sensor detects the distance to the nearest crumb. Equipped with both an angle and distance to the crumb, the network now learns behavior superior to the greedy algorithm. (Fig \ref{neroevolution:initial_results}

This makes sense, as agents can now leverage the information of how far away a crumb is. When a crumb is close by, it is likely that an agent will be able to capture it, improving the agent's fitness. When the crumb is further away, however, it is more likely that another agent will reach that crumb first. 

% % % % mention more crumb sensors result in slower learning, but not better results

\begin{figure}[!t]
\centering
\includegraphics[width=3.0in,height=2.0in]{./figures/neroevolution/times_5_.png}
\caption{Comparison of one and two-input networks to baseline greedy algorithm}
\label{neroevolution:initial_results}
\end{figure}

% % % % % % % % %

%Communication

We have establishes that training with rtNEAT can outperform the hardcoded greedy algorithm. However, agents are still entirely independent, not directly sharing information. An argument can be made that they are indirectly coordinating through the position of crumbs in the environment, but one wonders if better results can be achieved through direct communication.

The network design, shown in Fig. \ref{neroevolution:communication}, is straightforward. Inputs consist of the offset of the nearest crumb as well as that the nearest $n$ agents. Offsets are polar, consisting of an angular direction and a scalar magnitude. As before, the single output represents the angular direction the agent will turn to face. 

We compare results for different choices $n$. 

\begin{figure}[!t]
\centering
\includegraphics[width=3.0in]{./figures/neroevolution/comm_network.png}
\caption{Communicating network.}
\label{neroevolution:communication}
\end{figure}

As seen in Fig. \ref{neroevolution:communication_results}, the more neighbors the agent communicates with, the longer it takes to learn. After these networks were left to train for 500 generation, only the network with $n=1$ managed to equal the performance of the $n=0$ network. Fig. \ref{neroevolution:communication_results} shows that the one neighbor network catches up after about 20 episodes. Note that what both of these networks cap out at aproximately 400 crumbs per episode, there are still crumbs remaining at this time. (The total number of crumbs is about 435.) So while there is still room for improvement over the $n=0$ network, none of the communicating networks achieve better results. In fact the networks with higher $n$-values do \textit{worse} than the non-communicating network, plateauing at significantly lower crumb counts.

\begin{figure}[!t]
\centering
% % NOTE TO SELF: this spreadsheet doesnt exist yet, it was the latest experiment
\includegraphics[width=3.0in,height=2.0in]{./figures/neroevolution/comm_results.png}
\caption{Communicating network results}
\label{neroevolution:communication_results}
\end{figure}

%Shared fitness
Previous research by ??? and ??? has showed that sharing fitness functions between agents can improve multi-agent learning. In some cases, it is necessary to see any results [cite ???].

To explore this, we use a modification of the Roomba environment to share fitness between agents. Instead of being rewarded only when picking up a crumb, agents share the reward when one of their teammates finds a crumb. 

Fig ??? show the results for a non-communicating network ($n=0$) and a network which observes one neighbor ($n=1$). In both cases, the shared fitness networks perform abismully, showing no learning whatsoever.

%Evolving Communication


\begin{figure}[!t]
\centering
\includegraphics[width=3.0in]{./figures/neroevolution/emerg_comm_network.png}
\caption{Communicating network with evolved signals.}
\label{neroevolution:evolved_comunication}
\end{figure}


%neroevolution conclusion
Say something about how, surprisingly, communication can actually hurt network performance.

\section{Conclusions} \label{conclusion}
The conclusion goes here. In this paper, we investigate.

Promising future Works go here.

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/

\bibliographystyle{IEEEtran}
\bibliography{report}

% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

%\begin{thebibliography}{1}
%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%\end{thebibliography}




% that's all folks
\end{document}
